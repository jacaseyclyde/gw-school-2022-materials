{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11eb539c",
   "metadata": {},
   "source": [
    "# PTMCMCSampler & Bayesian Analyses using the PTA Likelihood Function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b895188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not have mpi4py package.\n",
      "Do not have acor package\n"
     ]
    }
   ],
   "source": [
    "import os, json, pickle\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import scipy.stats as sps\n",
    "\n",
    "import PTMCMCSampler.PTMCMCSampler as ptmcmc\n",
    "\n",
    "from emcee.autocorr import integrated_time\n",
    "\n",
    "import corner.corner as corner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458d6613",
   "metadata": {},
   "source": [
    "`cloudpickle` is a Python package which allows dynamically constructed clases to be pickled, unlike `pickle`. `Enterprise` uses class factories to make different signal classes for each of the different pulsars in a PTA, hence in order to pickle the full PTA one needs `cloudpickle`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "941db8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge cloudpickle\n",
    "# or install from PyPI\n",
    "# !pip install cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a919da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66f8a9",
   "metadata": {},
   "source": [
    "The easiest way to make the PTAs needed for this exercise is to make them with the script below. You will have to run the script on the command line to ensure that you use the correct ennvironment. \n",
    "\n",
    "``` Python\n",
    "python mk_pta_pkls.py \n",
    "```\n",
    "\n",
    "Check to see that the two pickle files have been saved in this directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bab53feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: vandy_3psr_fullpta_*.pkl: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls vandy_3psr_fullpta_*.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b56eba",
   "metadata": {},
   "source": [
    "If 2 different pickled files **don't** appear in this directory then you can alterantively copy/paste the script in `./mk_pta_pkls.py` in the cell below and delete the lines that pickle the 2 PTAs. We will need both loaded for the exercises below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b991aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: AstropyDeprecationWarning: The private astropy._erfa module has been made into its own package, pyerfa, which is a dependency of astropy and can be imported directly using \"import erfa\" [astropy._erfa]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "import os, json, pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from enterprise import constants as const\n",
    "from enterprise.pulsar import Pulsar\n",
    "from enterprise.signals import signal_base\n",
    "from enterprise.signals import gp_signals\n",
    "from enterprise.signals import gp_priors\n",
    "from enterprise.signals import parameter\n",
    "from enterprise.signals import selections\n",
    "\n",
    "from enterprise_extensions import blocks\n",
    "\n",
    "import cloudpickle\n",
    "\n",
    "# # using `enterprise_extensions`\n",
    "#\n",
    "# `enterprise_extensions` provides \"recipes\" for commonly used functionality in `enterprise`.\n",
    "#\n",
    "# Lets build a 3 pulsar PTA that we could use to search for a gravitational wave background.\n",
    "\n",
    "# ## load data\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# datafiles = {\n",
    "#     \"J1600-3053\":{\"par\":\"J1600-3053_EPTA_6psr.par\", \"tim\":\"J1600-3053_EPTA_6psr.tim\"},\n",
    "#     \"J2241-5236\":{\"par\":\"J2241-5236_PPTA_dr2.par\", \"tim\":\"J2241-5236_PPTA_dr2.tim\"},\n",
    "#     \"J2317+1439\":{\"par\":\"J2317+1439_NANOGrav_12y.par\", \"tim\":\"J2317+1439_NANOGrav_12y.tim\"},\n",
    "# }\n",
    "#\n",
    "# datadir = os.path.abspath(\"data\")\n",
    "#\n",
    "#\n",
    "# # In[3]:\n",
    "#\n",
    "#\n",
    "# # load in each pulsar and append it to a list\n",
    "# psrs = []\n",
    "# for pname, fdict in datafiles.items():\n",
    "#     pfile = os.path.join(datadir, fdict[\"par\"])\n",
    "#     tfile = os.path.join(datadir, fdict[\"tim\"])\n",
    "#     psrs.append(Pulsar(pfile, tfile))\n",
    "\n",
    "datadir = os.path.abspath(\"../Day_1/data\")\n",
    "\n",
    "with open(datadir+'/viper_3psr.pkl','rb') as fin:\n",
    "    psrs = pickle.load(fin)\n",
    "\n",
    "\n",
    "# (once again, we can safely ignore these `tempo2` warnings)\n",
    "\n",
    "# ## determine the PTA `Tspan`\n",
    "# When building a `PTA` using data from multiple pulsars it helps to have a common Fourier basis for all of the pulsars' red noise (and common red noise, like GWB).  The easy way to do this is to use the total time-span of all data to set the Fourier frequencies.\n",
    "#\n",
    "# `enterprise.signals.gp_signals.FourierBasisGP` can use an intput `Tspan` to figure out the frequencies, and several functions in `enterprise_extensions` can too.\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# calculate the total Tspan\n",
    "Tspan = np.max([pp.toas.max() - pp.toas.min() for pp in psrs])\n",
    "\n",
    "\n",
    "# ## generate an enterprise `PTA` for all three pulsars for a CRN analysis\n",
    "#\n",
    "# Each pulsar needs a different noise model.  For CRN analysis it is common to fix the WN parameters based on previous single pulsar noise runs.\n",
    "#\n",
    "# To speed up the likelihood calculation we can use the `enterprise.signals.gp_signals.MarginalizingTimingModel`, which breaks the GP coefficient marginalization into two steps.  The linear timing model is analytically marginalized first.  This reduces the size of the matrices that must be inverted at each likelihood evaluation.  Only the Fourier Basis GPs (RN, DM, GWB, ...) contribute.\n",
    "#\n",
    "# We're going to use a spatially correlated common red noies model with a powerlaw spectrum as our GWB.\n",
    "#\n",
    "# Let's start by building the parts of the model that all pulsars will include:\n",
    "#\n",
    "# * timing model\n",
    "# * red noise -- 30 frequency powerlaw -- `enterprise_extensions.blocks.red_noise_block`\n",
    "# * GWB -- 15 frequency powerlaw, Hellings-Downs correlated -- `enterprise_extensions.blocks.common_red_noise_block`\n",
    "#  * $\\log_{10} A \\rightarrow$ Uniform(-18, -13)\n",
    "#  * $\\gamma=13/3$\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# make the timing model signal\n",
    "tm = gp_signals.MarginalizingTimingModel(use_svd=True)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# make the RN signal\n",
    "rn = blocks.red_noise_block(\n",
    "    psd=\"powerlaw\", components=30,\n",
    "    Tspan=Tspan\n",
    ")\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# make the GWB signal\n",
    "gw = blocks.common_red_noise_block(\n",
    "    psd=\"powerlaw\", components=15,\n",
    "    gamma_val=13/3,\n",
    "    orf=\"hd\",\n",
    "    Tspan=Tspan\n",
    ")\n",
    "\n",
    "# make the Common Red Process signal\n",
    "crn = blocks.common_red_noise_block(\n",
    "    psd=\"powerlaw\", components=15,\n",
    "    gamma_val=13/3,\n",
    "    orf=None,\n",
    "    Tspan=Tspan\n",
    ")\n",
    "\n",
    "\n",
    "# Since each pulsar has a unique model, we'll store the three `SignalCollections` as a list.\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "# empty list to store each pulsar's \"signal\" model\n",
    "crn_sigs = []\n",
    "gw_sigs = []\n",
    "\n",
    "\n",
    "\n",
    "# ### generate an enterprise signal model for EPTA's J1600 pulsar\n",
    "#\n",
    "# In addition to the timing model, RN, and GWB, we need to include:\n",
    "#\n",
    "# * white noise -- fixed EFAC & EQUAD per backend (no ECORR)\n",
    "# * DM variations -- 100 frequency powerlaw DM GP\n",
    "#\n",
    "# These are easy to do using `enterprise_extensions.blocks`.\n",
    "#\n",
    "# For a GWB analysis it is common to hold the white noise parameters (EFAC/EQUAD/ECORR) fixed to some known value (as determined by a single pulsar analysis.\n",
    "# This reduces the number of parameters in the full PTA model.\n",
    "# `enterprise` accomplishes this by using the `parameter.Constant` class.\n",
    "# `enterprise_extensions.blocks.white_noise_block` has a boolean option to control this behavior.\n",
    "# We'll use `vary=False` for **fixed** WN.\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "# make the WN signal\n",
    "wn = blocks.white_noise_block(vary=False, inc_ecorr=False, select=\"backend\")\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "# make the DM GP signal\n",
    "dm = blocks.dm_noise_block(gp_kernel=\"diag\", psd=\"powerlaw\", components=100, Tspan=Tspan)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "# append J1600's SignalCOllection to the list\n",
    "gw_sigs.append(tm + wn + rn + dm + gw)\n",
    "crn_sigs.append(tm + wn + rn + dm + crn)\n",
    "\n",
    "\n",
    "# ### generate an enterprise signal model for PPTA's J2241 pulsar\n",
    "#\n",
    "# In addition to the timing model, RN, and GWB, we need to include:\n",
    "#\n",
    "# * white noise -- fixed EFAC & EQUAD per backend (no ECORR)\n",
    "# * DM variations -- 100 frequency powerlaw DM GP\n",
    "# * band noise -- 30 frequency powerlaw in the 20cm band\n",
    "#\n",
    "# We can reuse the same `wn` and `dm` signals from before.\n",
    "#\n",
    "# To implement band noise we need a `enterprise.signal.selections.Selection`.\n",
    "# A selection function takes the `dict` of TOA flags and flagvals as input.\n",
    "# It returns a `dict` whose keys are the flagvals to select and mask (array of True/False) telling which TOAs have that flag.\n",
    "#\n",
    "# There's a built in `by_band` selection function, but that applies band noise to **all** bands.\n",
    "# We only want to apply this model to TOAs in the 20cm band, so we need a selection function that returns a `dict` with one key and a mask for that flagval.\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "def band_20cm(flags):\n",
    "    \"\"\"function to select TOAs in 20cm band (-B 20CM)\"\"\"\n",
    "    flagval = \"20CM\"\n",
    "    return {flagval: flags[\"B\"] == flagval}\n",
    "\n",
    "by_band_20cm = selections.Selection(band_20cm)\n",
    "\n",
    "\n",
    "# There's no band noise block in `enterprise_extensions` but we can make a Fourier basis GP with the appropriate selection the old fashioned way!\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# band noise parameters\n",
    "BN_logA = parameter.Uniform(-20, -11)\n",
    "BN_gamma = parameter.Uniform(0, 7)\n",
    "\n",
    "# band noise powerlaw prior\n",
    "powlaw = gp_priors.powerlaw(log10_A=BN_logA, gamma=BN_gamma)\n",
    "\n",
    "# make band noise signal (don't forget the name!)\n",
    "bn = gp_signals.FourierBasisGP(\n",
    "    powlaw,\n",
    "    components=30,\n",
    "    Tspan=Tspan,\n",
    "    selection=by_band_20cm,\n",
    "    name=\"band_noise\"\n",
    ")\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "# append J2241's SignalCOllection to the list\n",
    "gw_sigs.append(tm + wn + rn + bn + dm + gw)\n",
    "crn_sigs.append(tm + wn + rn + bn + dm + crn)\n",
    "\n",
    "\n",
    "# ### generate an enterprise signal model for NANOGrav's J2317 pulsar\n",
    "#\n",
    "# In addition to the timing model, RN, and GWB, we need to include:\n",
    "#\n",
    "# * white noise -- fixed EFAC, EQUAD, **and ECORR** per backend\n",
    "#\n",
    "# Remember there is no DM variations model, because DMX is already in the timing model for NANOGrav's 12.5yr data release\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "# make WN signal (now with ECORR!)\n",
    "wn_ec = blocks.white_noise_block(vary=False, inc_ecorr=True, select=\"backend\")\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "# append J2317's SignalCOllection to the list\n",
    "gw_sigs.append(tm + wn_ec + rn + gw)\n",
    "crn_sigs.append(tm + wn_ec + rn + crn)\n",
    "\n",
    "\n",
    "# ## put the three pulsars together into a `PTA` object\n",
    "#\n",
    "# We can instantiate a PTA object with a list of three pulsar models.\n",
    "# We simply feed each `Pulsar` to its `SignalCollection`, and then pass the whole list of instantiated models to `signal_base.PTA`.\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "pta_gw = signal_base.PTA([ss(pp) for ss,pp in zip(gw_sigs, psrs)])\n",
    "pta_crn = signal_base.PTA([ss(pp) for ss,pp in zip(crn_sigs, psrs)])\n",
    "\n",
    "# ### load noise dictionary\n",
    "#\n",
    "# At this point we never actually told `enterprise` what to use for the fixed the WN parameters.\n",
    "# We can use `PTA.set_default_params` to pass in the correct WN values from a `dict`.\n",
    "#\n",
    "# First we'll load the dictionary, which is stored as a `.json` file in the `data/` directory\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "nfile = os.path.join(datadir, \"viper_3psr_noise.json\")\n",
    "with open(nfile, \"r\") as f:\n",
    "    noisedict = json.load(f)\n",
    "\n",
    "# set the fixed WN params\n",
    "pta_gw.set_default_params(noisedict)\n",
    "pta_crn.set_default_params(noisedict)\n",
    "\n",
    "with open('./vandy_3psr_fullpta_gwb.pkl','wb') as fout:\n",
    "    cloudpickle.dump(pta_gw,fout)\n",
    "\n",
    "with open('./vandy_3psr_fullpta_crn.pkl','wb') as fout:\n",
    "    cloudpickle.dump(pta_crn,fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b239cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./vandy_3psr_fullpta_crn.pkl','rb') as fin:\n",
    "    pta_crn = cloudpickle.load(fin)\n",
    "    \n",
    "with open('./vandy_3psr_fullpta_gwb.pkl','rb') as fin:\n",
    "    pta_gwb = cloudpickle.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9664e970",
   "metadata": {},
   "source": [
    "### test the log-likelihoods and log-priors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df0863e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random point\n",
    "x0 = {pp.name:pp.sample() for pp in pta_gwb.params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21cc3737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GWB PTA:  225313.50361250053 -26.245102719469088\n",
      "CRN PTA:  225313.50360684513 -26.245102719469088\n"
     ]
    }
   ],
   "source": [
    "# calculate logL and logPr\n",
    "print('GWB PTA: ',pta_gwb.get_lnlikelihood(x0), pta_gwb.get_lnprior(x0))\n",
    "print('CRN PTA: ',pta_crn.get_lnlikelihood(x0), pta_crn.get_lnprior(x0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5db2a",
   "metadata": {},
   "source": [
    "## The Challenges of the PTA Likelihood Function\n",
    "There are a number of reasons why it's challenging to sample your way to a reasonable posterior or evidence with starting from a PTA likelihood:\n",
    "\n",
    "1. The continual growth of PTA datasets and the need to do full dataset cross-correlation calculations slows down the likelihood evalution. See Steve's talk from yesterday for scaling of the likelihood evaluation.\n",
    "2. Large parameter spaces combined with slow likelihood evaluations considerably slows down the amount of time to convergence.\n",
    "3. Lack of derivative information from `enterprise` functions makes the use of Hamiltonian Monte Carlo techniques challenging.\n",
    "4. Always remember that things are this slow and we are _marginalizing over the timing model_ and _setting the white noise parameters constant_! (i.e. stay humble and keep looking for better ways to sample :-)\n",
    "\n",
    "Please keep looking and testing new algorithms and samplers as the appear in the literature and in the open source code community!!\n",
    "\n",
    "## Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6221ad3",
   "metadata": {},
   "source": [
    "### MCMC\n",
    "\n",
    "`enterprise_extensions.sampler.setup_sampler()` returns a `PTMCMCSampler` object.\n",
    "The MCMC sampler can be tuned to improve performance.\n",
    "The defaults of `setup_sampler` are often fine, but suboptimal.\n",
    "One can fine improved acceptance and convergence, but adjusting the inputs to `setup_sampler` or setting up the sampler __manually__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8421fb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: version mismatch between CFITSIO header (v4.000999999999999) and linked library (v4.01).\n",
      "\n",
      "\n",
      "WARNING: version mismatch between CFITSIO header (v4.000999999999999) and linked library (v4.01).\n",
      "\n",
      "\n",
      "WARNING: version mismatch between CFITSIO header (v4.000999999999999) and linked library (v4.01).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import enterprise_extensions.sampler as Sampler\n",
    "from enterprise_extensions.sampler import JumpProposal as jp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33e64181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J1600-3053_dm_gp_gamma',\n",
       " 'J1600-3053_dm_gp_log10_A',\n",
       " 'J1600-3053_red_noise_gamma',\n",
       " 'J1600-3053_red_noise_log10_A',\n",
       " 'J2241-5236_band_noise_20CM_gamma',\n",
       " 'J2241-5236_band_noise_20CM_log10_A',\n",
       " 'J2241-5236_dm_gp_gamma',\n",
       " 'J2241-5236_dm_gp_log10_A',\n",
       " 'J2241-5236_red_noise_gamma',\n",
       " 'J2241-5236_red_noise_log10_A',\n",
       " 'J2317+1439_red_noise_gamma',\n",
       " 'J2317+1439_red_noise_log10_A',\n",
       " 'gw_log10_A']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pta_crn.param_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a417e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = len(pta_crn.param_names)\n",
    "cov_new = np.diag(np.ones(ndim) * 0.1**2)  # param covariance, used for jump proposals\n",
    "groups = [[0,1],[2,3],[0,1,2,3],[4,5],]\n",
    "sampler0 = ptmcmc.PTSampler(ndim=ndim,  # necessary to setup\n",
    "                            logl=pta_crn.get_lnlikelihood,  # necessary to setup\n",
    "                            logp=pta_crn.get_lnprior,  # necessary to setup\n",
    "                            cov=cov_new,  # necessary to setup\n",
    "                            groups = groups,\n",
    "                            outDir='chains0/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be4ce6e",
   "metadata": {},
   "source": [
    "`PTMCMCSampler` contains a few built-in jump proposals including:\n",
    "```\n",
    "AM => Adaptive Metroplis\n",
    "SCAM => Single Component Adaptive Metroplis\n",
    "DE => Differential Evolution\n",
    "```\n",
    "The first two use the covariance matrix, which is iteratively recalculated, to make predictions about various parameters. The groups are used by the `AM` proposals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21fac5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "JP = jp(pta_crn)\n",
    "sampler0.addProposalToCycle(JP.draw_from_gwb_log_uniform_distribution, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23d41123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an initial sample and run the sampler!\n",
    "p0 = np.hstack(list(x0.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32e740a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 5_000\n",
    "T = 2\n",
    "Nsamp = 20_000\n",
    "\n",
    "N = T * Nsamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60ff1ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacaseyclyde/miniconda3/envs/viper/lib/python3.9/site-packages/enterprise/signals/parameter.py:62: RuntimeWarning: divide by zero encountered in log\n",
      "  logpdf = np.log(self.prior(value, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 12.50 percent in 14.188779 s Acceptance rate = 0.21925Adding DE jump with weight 20\n",
      "Finished 97.50 percent in 131.962667 s Acceptance rate = 0.251538\n",
      "Run Complete\n"
     ]
    }
   ],
   "source": [
    "sampler0.sample(p0,\n",
    "                Niter=N,\n",
    "                thin=T,\n",
    "                burn=B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ca4d3",
   "metadata": {},
   "source": [
    "### `enterprise_extensions` sampler\n",
    "\n",
    "Check the output to see what is generated and plot a trace of the log-Posterior\n",
    "\n",
    "* set the output directory\n",
    "* specify yourself as the \"human\" running the job\n",
    "\n",
    "* remember the sampler takes a `numpy.ndarray` for the starting location, not a `dict`\n",
    "\n",
    "* set `burn=5000` (the DE buffer) DO NOT confuse with the usual use of `burn`\n",
    "* set `thin=2` (save every other sample)\n",
    "* collect `20000` samples (if we thin by 2, we'll need to run for `Niter=40000`!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f141099a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding red noise prior draws...\n",
      "\n",
      "Adding DM GP noise prior draws...\n",
      "\n",
      "Adding GWB uniform distribution draws...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setup the sampler\n",
    "sampler = Sampler.setup_sampler(pta_crn, outdir='crn_pta', human=\"jsh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d661173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 12.50 percent in 13.937409 s Acceptance rate = 0.3014Adding DE jump with weight 20\n",
      "Finished 97.50 percent in 127.132059 s Acceptance rate = 0.322256\n",
      "Run Complete\n"
     ]
    }
   ],
   "source": [
    "sampler.sample(p0,\n",
    "               Niter=N,\n",
    "               burn=B,\n",
    "               thin=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f7514",
   "metadata": {},
   "source": [
    "### Jump Proposals\n",
    "One can add more jump proposals. The `JumpProposal` class is available to use for more various new proposals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73922d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_from_band_noise_prior(self, x, iter, beta):\n",
    "\n",
    "        q = x.copy()\n",
    "        lqxy = 0\n",
    "\n",
    "        # draw parameter from signal model\n",
    "        name = 'band_noise'\n",
    "        par = np.random.choice([p for p in self.pnames if name in p])\n",
    "        idx = list(self.pnames).index(par)\n",
    "        param = self.params[idx]\n",
    "\n",
    "        q[self.pmap[str(param)]] = np.random.uniform(param.prior._defaults['pmin'], param.prior._defaults['pmax'])\n",
    "\n",
    "        # forward-backward jump probability\n",
    "        lqxy = (param.get_logpdf(x[self.pmap[str(param)]]) -\n",
    "                param.get_logpdf(q[self.pmap[str(param)]]))\n",
    "\n",
    "        return q, float(lqxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ead331ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding red noise prior draws...\n",
      "\n",
      "Adding DM GP noise prior draws...\n",
      "\n",
      "Adding GWB uniform distribution draws...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler2 = Sampler.setup_sampler(pta_crn, outdir=\"crn_pta_jp\", human=\"jsh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09928c12",
   "metadata": {},
   "source": [
    "**Note:** Below we are changing the generic `Sampler` class not just an instance of the class by adding this new jump proposal as an attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "968f5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sampler.JumpProposal.draw_from_band_noise_prior = draw_from_band_noise_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e09ab7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler2.addProposalToCycle(sampler2.jp.draw_from_band_noise_prior, 40) # Number is the relative weight of proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "510195d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dm_gp': [J1600-3053_dm_gp_log10_A:Uniform(pmin=-20, pmax=-11),\n",
       "  J1600-3053_dm_gp_gamma:Uniform(pmin=0, pmax=7),\n",
       "  J2241-5236_dm_gp_log10_A:Uniform(pmin=-20, pmax=-11),\n",
       "  J2241-5236_dm_gp_gamma:Uniform(pmin=0, pmax=7)],\n",
       " 'ecorr_sherman-morrison': [],\n",
       " 'marginalizing linear timing model': [],\n",
       " 'measurement_noise': [],\n",
       " 'red noise': [J2241-5236_red_noise_log10_A:Uniform(pmin=-20, pmax=-11),\n",
       "  gw_log10_A:Uniform(pmin=-18, pmax=-14),\n",
       "  J2317+1439_red_noise_gamma:Uniform(pmin=0, pmax=7),\n",
       "  J2241-5236_red_noise_gamma:Uniform(pmin=0, pmax=7),\n",
       "  J2241-5236_band_noise_20CM_gamma:Uniform(pmin=0, pmax=7),\n",
       "  J2317+1439_red_noise_log10_A:Uniform(pmin=-20, pmax=-11),\n",
       "  J1600-3053_red_noise_log10_A:Uniform(pmin=-20, pmax=-11),\n",
       "  J2241-5236_band_noise_20CM_log10_A:Uniform(pmin=-20, pmax=-11),\n",
       "  J1600-3053_red_noise_gamma:Uniform(pmin=0, pmax=7)]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler2.jp.snames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a126c255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 12.50 percent in 14.066790 s Acceptance rate = 0.28525Adding DE jump with weight 20\n",
      "Finished 97.50 percent in 142.800007 s Acceptance rate = 0.301795\n",
      "Run Complete\n"
     ]
    }
   ],
   "source": [
    "sampler2.sample(p0,\n",
    "                Niter=N,\n",
    "                burn=B,\n",
    "                thin=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dd13a4",
   "metadata": {},
   "source": [
    "### Empirical Distributions\n",
    "_Empirical distributions_ are probablity density functions based on samples from a previous MCMC analysis. Here we will use the noise runs done bye Paul Baker to obtain the white noise parameters that are being set constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93bacb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enterprise_extensions.empirical_distr import make_empirical_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bd886ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import la_forge.slices as sl\n",
    "import la_forge.core as co\n",
    "import la_forge.diagnostics as dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd6a3e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf viper_3psr_noiseruns.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9864e20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J2317+1439/chain_1.txt is loaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "to_get = []\n",
    "\n",
    "for psr in pta_crn.pulsars:\n",
    "    to_get.append([par for par in pta_crn.param_names if psr in par and 'J1600-3053_dm_gp' not in par])\n",
    "    \n",
    "slc = sl.SlicesCore(slicedirs=pta_crn.pulsars,\n",
    "                    pars2pull=to_get,params=[it for sub in to_get for it in sub ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca0a6bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['J1600-3053_red_noise_gamma', 'J1600-3053_red_noise_log10_A'],\n",
       " ['J1600-3053_red_noise_log10_A', 'J2241-5236_band_noise_20CM_gamma'],\n",
       " ['J2241-5236_band_noise_20CM_gamma', 'J2241-5236_band_noise_20CM_log10_A'],\n",
       " ['J2241-5236_band_noise_20CM_log10_A', 'J2241-5236_dm_gp_gamma'],\n",
       " ['J2241-5236_dm_gp_gamma', 'J2241-5236_dm_gp_log10_A']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params2d = []\n",
    "for ii in range(int(len(slc.params)//2)):\n",
    "    params2d.append([slc.params[ii],slc.params[ii+1]])\n",
    "    \n",
    "params2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef7e503c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<enterprise_extensions.empirical_distr.EmpiricalDistribution2D at 0x183eebaf0>,\n",
       " <enterprise_extensions.empirical_distr.EmpiricalDistribution2D at 0x1842e5f70>,\n",
       " <enterprise_extensions.empirical_distr.EmpiricalDistribution2D at 0x18509f8b0>,\n",
       " <enterprise_extensions.empirical_distr.EmpiricalDistribution2D at 0x18509f2e0>,\n",
       " <enterprise_extensions.empirical_distr.EmpiricalDistribution2D at 0x18509fd30>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_empirical_distributions(pta_crn,\n",
    "                             paramlist=params2d,# List of 2 element lists to make 2d emp dists\n",
    "                             params=slc.params, #List of params in chain array\n",
    "                             chain=slc.chain,\n",
    "                             filename='viper_3psr_pta_emp_distr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25b425a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending empirical distributions to priors...\n",
      "\n",
      "Attempting to add empirical proposals...\n",
      "\n",
      "Adding red noise prior draws...\n",
      "\n",
      "Adding DM GP noise prior draws...\n",
      "\n",
      "Adding GWB uniform distribution draws...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler3 = Sampler.setup_sampler(pta_crn, \n",
    "                                 outdir=\"crn_pta_ed\", \n",
    "                                 human=\"jsh\",\n",
    "                                 empirical_distr='viper_3psr_pta_emp_distr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fd74678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 12.50 percent in 16.391260 s Acceptance rate = 0.30865Adding DE jump with weight 20\n",
      "Finished 97.50 percent in 128.510320 s Acceptance rate = 0.324436\n",
      "Run Complete\n"
     ]
    }
   ],
   "source": [
    "sampler3.sample(\n",
    "    p0, Niter=N,\n",
    "    burn=B, thin=T\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc85bfa",
   "metadata": {},
   "source": [
    "## Product Space Sampling with the `HyperModel` submodule\n",
    "\n",
    "[Product space sampling](https://academic.oup.com/mnras/article/455/3/2461/1069531) allows one to calculate Bayes factors without using the Savage-Dickey approximation or calculating evidences or posterior odds ratios. Multiple models are concatenated into one large HyperModel along with a model parameter which selects which model likelihood is evaluated in each iteration. The relative number of samples in for the `nmodel` parameter can be interpreted as the odds ratio between the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66391be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enterprise_extensions.hypermodel import HyperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ccbd42f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptas = {0:pta_crn,1:pta_gwb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3198be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = HyperModel(ptas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb1e418",
   "metadata": {},
   "source": [
    "**Note:** The `log_weights` flag can be used to put models on an even footing when one model is preferred over another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f58d03e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending empirical distributions to priors...\n",
      "\n",
      "Adding empirical proposals...\n",
      "\n",
      "Adding red noise prior draws...\n",
      "\n",
      "Adding DM GP noise prior draws...\n",
      "\n",
      "Adding GWB uniform distribution draws...\n",
      "\n",
      "Adding gw param prior draws...\n",
      "\n",
      "Adding nmodel uniform distribution draws...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacaseyclyde/miniconda3/envs/viper/lib/python3.9/site-packages/numpy/lib/arraysetops.py:272: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ar = np.asanyarray(ar)\n"
     ]
    }
   ],
   "source": [
    "hsampler = hm.setup_sampler(outdir='hypermodel',empirical_distr='viper_3psr_pta_emp_distr.pkl',human='jsh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1689137e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 12.50 percent in 50.437612 s Acceptance rate = 0.3928Adding DE jump with weight 20\n",
      "Finished 97.50 percent in 419.973111 s Acceptance rate = 0.400615\n",
      "Run Complete\n"
     ]
    }
   ],
   "source": [
    "x0 = hm.initial_sample()\n",
    "hsampler.sample(x0,\n",
    "                Niter=N,\n",
    "                burn=B,\n",
    "                thin=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb561ade",
   "metadata": {},
   "source": [
    "# Things to work on during the hack time\n",
    "\n",
    "You don't need to do all of these, and you don't need to do them in order.  Pick one (or more) to try.\n",
    "MCMC's can take a **long** time to run.  You may want to set up a notebook, then wait to run it overnight.\n",
    "\n",
    "While the MCMC is running you can use the post processing notebook and read in the samples using `la_forge` to check on progress.\n",
    "The number of samples it will take to have well converged posteriors will depend on the specific model.\n",
    "Using these settings in `sampler.sample` will run for 100k samples, saving every tenth.\n",
    "That may be enough...\n",
    "* `Niter=100_000`\n",
    "* `burn=20_000`\n",
    "* `thin=10`\n",
    "Models with more parameters will require more samples to converge.\n",
    "\n",
    "\n",
    "## questions:\n",
    "\n",
    "* What changes to the \"vanilla\" sampler can be made to get better convergence?\n",
    "* Do the proposal acceptances change when new proposals are added into the mix?\n",
    "* If a model is weighted in the `HyperModel` class how does one \"unweight\" the models to compare the odds ratios?\n",
    "\n",
    "\n",
    "## ideas:\n",
    "\n",
    "* Try parallel tempering on the same `CRN` model above and compare the Gelman-Rubin statistic and autocorrelation lengths given the same number of samples. \n",
    "\n",
    "* Use the `HyperModel` class to find the odds ratio for a common red process versus a model _without_ any common signal, correlated or otherwise. You will need to edit the script that makes `PTA` classes by building a PTA without a common signal. \n",
    "\n",
    "* Choose another sampler, e.g.,[dynesty](), [ultranest](), [zeus](https://zeus-mcmc.readthedocs.io/en/latest/), [emcee](https://emcee.readthedocs.io/en/stable/)... (**Note:** These will not work easily for full PTAs, but should work on the simplified examples used above.)\n",
    "\n",
    "* Pull recent public data from your favorite PTA and set up a `PTA` object to perform an **analysis of your choice**.  Here are `.pkl` files containing a list of `enterprise` `Pulsar` objects for three recent data releases:\n",
    " * [EPTA 6PSR](https://drive.google.com/file/d/1MyZX7ox_8TlRUhgk47NirNYcWfEz5ron/view?usp=sharing)\n",
    " * [PPTA DR2](https://drive.google.com/file/d/1at5S_ydfqGV2x0PzF4eCO_BXhQjfamKX/view?usp=sharing)\n",
    " * [NANOGrav 12.5y](https://drive.google.com/file/d/1eWNLgPOm7mYKAt3LYY_YIb1i19_n03xD/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335ae4d5",
   "metadata": {},
   "source": [
    "## Parallel Tempering\n",
    "`PTMCMCSampler` has [PT functionality](https://academic.oup.com/gji/article/196/1/357/585739) available, but one needs to use MPI (Message Passing Interface), which is not compatible with Jupyter notebooks, in order to use it. If your laptop has multiple cores, or you have access to a cluster you can try a PT run. First you need to write a Python script (hint: start by converting this notebook into a script using `jupyter nbconvert --to=script sampling_the_enterprise_likelihood.ipynb`.) Second you would use the following command \n",
    "```\n",
    "mpiexec -np 4 python your_script.py\n",
    "```\n",
    "The `-np 4` tells MPI to use 4 cores. You can change this number if you'd like. When the sampler detectes that there is an MPI environment it will automatically initiate parallel tempering. I'd recommend setting the `sampler.sample` flag `writeHotChains=True`, which will write the hot chains to disk. This is useful for knowing that PT is actually working. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6388e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
